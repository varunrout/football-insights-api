{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5efb2d",
   "metadata": {},
   "source": [
    "# Football Data Management Strategy\n",
    "\n",
    "## Storage Strategy for Multiple Competitions\n",
    "\n",
    "Since you can't use CSV or a database, here's a strategy for managing data for 10 different competitions:\n",
    "\n",
    "1. **Hierarchical Memory Cache**\n",
    "   - Create a structured in-memory cache using dictionaries\n",
    "   - Organize by competition → season → match → data type (events/360)\n",
    "   - Implement selective loading and memory management\n",
    "\n",
    "2. **Serialization for Persistence**\n",
    "   - Use Python's pickle or joblib to save processed data\n",
    "   - Store in hierarchical file structure for easy retrieval\n",
    "   - Load only what's needed when performing analysis\n",
    "\n",
    "## Implementation Approach\n",
    "\n",
    "Here's a class to manage the football data ecosystem:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c26ddbf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:07:25.042671Z",
     "start_time": "2025-05-24T22:07:12.172753Z"
    }
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from statsbombpy import sb\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import requests\n",
    "\n",
    "class FootballDataManager:\n",
    "    \"\"\"\n",
    "    Manages football data across multiple competitions with in-memory caching\n",
    "    and serialization capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"data_cache\"):\n",
    "        \"\"\"\n",
    "        Initialize the FootballDataManager.\n",
    "        \n",
    "        Args:\n",
    "            cache_dir: Directory for serialized data storage\n",
    "        \"\"\"\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # In-memory cache structure\n",
    "        self.competitions_cache = None\n",
    "        self.matches_cache = {}  # {competition_id_season_id: matches_df}\n",
    "        self.events_cache = {}   # {match_id: events_df}\n",
    "        self.frames_cache = {}   # {match_id: freeze_frame_df}\n",
    "        \n",
    "    def get_competitions(self, force_refresh: bool = False, \n",
    "                         only_with_360: bool = True, \n",
    "                         exclude_women: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get available competitions, with optional filtering.\n",
    "        \n",
    "        Args:\n",
    "            force_refresh: If True, fetches from API even if cached\n",
    "            only_with_360: Filter to competitions with 360 data available\n",
    "            exclude_women: Filter out women's competitions\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame of competitions\n",
    "        \"\"\"\n",
    "        if self.competitions_cache is None or force_refresh:\n",
    "            competitions = sb.competitions()\n",
    "            \n",
    "            # Apply filters if specified\n",
    "            if only_with_360:\n",
    "                competitions = competitions[competitions['match_available_360'].notna()]\n",
    "            if exclude_women:\n",
    "                competitions = competitions[competitions['competition_gender'] != 'women']\n",
    "                \n",
    "            self.competitions_cache = competitions\n",
    "            \n",
    "        return self.competitions_cache\n",
    "    \n",
    "    def get_matches(self, competition_id: int, season_id: int, \n",
    "                   force_refresh: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get matches for a specific competition and season.\n",
    "        \n",
    "        Args:\n",
    "            competition_id: The competition ID\n",
    "            season_id: The season ID\n",
    "            force_refresh: If True, fetches from API even if cached\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame of matches\n",
    "        \"\"\"\n",
    "        cache_key = f\"{competition_id}_{season_id}\"\n",
    "        \n",
    "        if cache_key not in self.matches_cache or force_refresh:\n",
    "            matches = sb.matches(competition_id=competition_id, season_id=season_id)\n",
    "            self.matches_cache[cache_key] = matches\n",
    "            \n",
    "        return self.matches_cache[cache_key]\n",
    "    \n",
    "    def get_events(self, match_id: int, force_refresh: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get events data for a specific match.\n",
    "        \n",
    "        Args:\n",
    "            match_id: The match ID\n",
    "            force_refresh: If True, fetches from API even if cached\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame of events\n",
    "        \"\"\"\n",
    "        if match_id not in self.events_cache or force_refresh:\n",
    "            events = sb.events(match_id=match_id)\n",
    "            # Process events to standardized format\n",
    "            events_df = self._process_events(events)\n",
    "            self.events_cache[match_id] = events_df\n",
    "            \n",
    "        return self.events_cache[match_id]\n",
    "    \n",
    "    def get_freeze_frames(self, match_id: int, force_refresh: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get 360 freeze frame data for a specific match.\n",
    "        \n",
    "        Args:\n",
    "            match_id: The match ID\n",
    "            force_refresh: If True, fetches from API even if cached\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame of freeze frame data or empty DataFrame if not available\n",
    "        \"\"\"\n",
    "        if match_id not in self.frames_cache or force_refresh:\n",
    "            try:\n",
    "                frames = sb.frames(match_id=match_id)\n",
    "                self.frames_cache[match_id] = frames\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                # Handle 404 errors when 360 data is not available for this match\n",
    "                if e.response.status_code == 404:\n",
    "                    print(f\"360 data not available for match {match_id}. Returning empty DataFrame.\")\n",
    "                    # Create empty DataFrame with expected structure\n",
    "                    self.frames_cache[match_id] = pd.DataFrame({\n",
    "                        'id': [], 'visible_area': [], 'match_id': [],\n",
    "                        'teammate': [], 'actor': [], 'keeper': [], 'location': []\n",
    "                    })\n",
    "                else:\n",
    "                    # Re-raise if it's not a 404 error\n",
    "                    raise\n",
    "            \n",
    "        return self.frames_cache[match_id]\n",
    "    \n",
    "    def _process_events(self, events_data) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process events data into a standardized DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            events_data: Raw events data from statsbombpy\n",
    "            \n",
    "        Returns:\n",
    "            Processed DataFrame\n",
    "        \"\"\"\n",
    "        # Define standard columns based on your current implementation\n",
    "        columns = ['location', 'pass_end_location', 'shot_end_location', \n",
    "                  'player', 'team', 'type', 'possession', 'play_pattern',\n",
    "                  'minute', 'second', 'period', 'timestamp', 'id',\n",
    "                  'match_id', 'pass_outcome', 'shot_outcome', 'shot_statsbomb_xg',\n",
    "                  'possession_team', 'position', 'shot_freeze_frame']\n",
    "        \n",
    "        # Create DataFrame based on input type\n",
    "        if isinstance(events_data, dict):\n",
    "            df = pd.DataFrame.from_dict(events_data, orient='index')\n",
    "        else:\n",
    "            df = pd.DataFrame(events_data)\n",
    "        \n",
    "        # Ensure all columns exist\n",
    "        for col in columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        # Set index to event id\n",
    "        if 'id' in df.columns:\n",
    "            df.set_index('id', inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def serialize_data(self, data_type: str, identifier: str, data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Serialize data to disk.\n",
    "        \n",
    "        Args:\n",
    "            data_type: Type of data ('competitions', 'matches', 'events', 'frames')\n",
    "            identifier: Identifier for the data (e.g., match_id or competition_id)\n",
    "            data: DataFrame to serialize\n",
    "        \"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        directory = self.cache_dir / data_type\n",
    "        directory.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Save data\n",
    "        file_path = directory / f\"{identifier}.pkl\"\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    \n",
    "    def load_serialized_data(self, data_type: str, identifier: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load serialized data from disk.\n",
    "        \n",
    "        Args:\n",
    "            data_type: Type of data ('competitions', 'matches', 'events', 'frames')\n",
    "            identifier: Identifier for the data (e.g., match_id or competition_id)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame or None if file doesn't exist\n",
    "        \"\"\"\n",
    "        file_path = self.cache_dir / data_type / f\"{identifier}.pkl\"\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            return None\n",
    "            \n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def prepare_data_for_analysis(self, competition_ids: List[int] = None, \n",
    "                                max_matches_per_competition: int = 5) -> Dict:\n",
    "        \"\"\"\n",
    "        Prepare a comprehensive dataset for analysis across multiple competitions.\n",
    "        \n",
    "        Args:\n",
    "            competition_ids: List of competition IDs to include (None = use all)\n",
    "            max_matches_per_competition: Maximum matches to include per competition\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with analysis-ready data\n",
    "        \"\"\"\n",
    "        analysis_data = {\n",
    "            'competitions': {},\n",
    "            'summary': {\n",
    "                'total_competitions': 0,\n",
    "                'total_matches': 0,\n",
    "                'total_events': 0,\n",
    "                'matches_with_360': 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Get competitions\n",
    "        competitions = self.get_competitions()\n",
    "        \n",
    "        # Filter to requested competitions if specified\n",
    "        if competition_ids:\n",
    "            competitions = competitions[competitions['competition_id'].isin(competition_ids)]\n",
    "        \n",
    "        # Process each competition\n",
    "        for _, comp in competitions.iterrows():\n",
    "            comp_id = comp['competition_id'] \n",
    "            season_id = comp['season_id']\n",
    "            \n",
    "            # Get matches for this competition\n",
    "            matches = self.get_matches(comp_id, season_id)\n",
    "            \n",
    "            # Limit number of matches if needed\n",
    "            match_subset = matches.head(max_matches_per_competition)\n",
    "            \n",
    "            # Store competition data\n",
    "            analysis_data['competitions'][comp_id] = {\n",
    "                'name': comp['competition_name'],\n",
    "                'season': comp['season_name'],\n",
    "                'matches': {}\n",
    "            }\n",
    "            \n",
    "            # Process each match\n",
    "            for _, match in match_subset.iterrows():\n",
    "                match_id = match['match_id']\n",
    "                \n",
    "                # Get events for this match\n",
    "                events = self.get_events(match_id)\n",
    "                \n",
    "                # Attempt to get frames for this match (may be empty)\n",
    "                frames = self.get_freeze_frames(match_id)\n",
    "                \n",
    "                # Store match data\n",
    "                analysis_data['competitions'][comp_id]['matches'][match_id] = {\n",
    "                    'home_team': match['home_team'],\n",
    "                    'away_team': match['away_team'],\n",
    "                    'score': f\"{match['home_score']}-{match['away_score']}\",\n",
    "                    'events': events,\n",
    "                    'freeze_frames': frames,\n",
    "                    'has_360_data': not frames.empty\n",
    "                }\n",
    "                \n",
    "                # Increment 360 data counter if available\n",
    "                if not frames.empty:\n",
    "                    analysis_data['summary']['matches_with_360'] += 1\n",
    "            \n",
    "            # Update summary counts\n",
    "            analysis_data['summary']['total_competitions'] += 1\n",
    "            analysis_data['summary']['total_matches'] += len(match_subset)\n",
    "            analysis_data['summary']['total_events'] += sum(\n",
    "                len(m['events']) for m in analysis_data['competitions'][comp_id]['matches'].values()\n",
    "            )\n",
    "        \n",
    "        return analysis_data\n",
    "    \n",
    "    def save_analysis_dataset(self, analysis_data: Dict, dataset_name: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Save complete analysis dataset to disk in a structured format.\n",
    "        \n",
    "        Args:\n",
    "            analysis_data: Analysis dataset dictionary returned by prepare_data_for_analysis\n",
    "            dataset_name: Optional name for this dataset (defaults to timestamp)\n",
    "            \n",
    "        Returns:\n",
    "            Path to saved dataset directory\n",
    "        \"\"\"\n",
    "        # Create a unique name for this dataset if not provided\n",
    "        if dataset_name is None:\n",
    "            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dataset_name = f\"analysis_{timestamp}\"\n",
    "        \n",
    "        # Create directory for this dataset\n",
    "        dataset_dir = self.cache_dir / dataset_name\n",
    "        dataset_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Save dataset metadata/summary\n",
    "        summary = {\n",
    "            'created_at': datetime.datetime.now().isoformat(),\n",
    "            'summary': analysis_data['summary'],\n",
    "            'competition_ids': list(analysis_data['competitions'].keys())\n",
    "        }\n",
    "        with open(dataset_dir / 'summary.pkl', 'wb') as f:\n",
    "            pickle.dump(summary, f)\n",
    "            \n",
    "        # Create competitions directory\n",
    "        competitions_dir = dataset_dir / 'competitions'\n",
    "        competitions_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save each competition's data\n",
    "        for comp_id, comp_data in analysis_data['competitions'].items():\n",
    "            # Create competition directory\n",
    "            comp_dir = competitions_dir / str(comp_id)\n",
    "            comp_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Save competition metadata\n",
    "            comp_meta = {\n",
    "                'name': comp_data['name'],\n",
    "                'season': comp_data['season'],\n",
    "                'match_ids': list(comp_data['matches'].keys())\n",
    "            }\n",
    "            with open(comp_dir / 'metadata.pkl', 'wb') as f:\n",
    "                pickle.dump(comp_meta, f)\n",
    "            \n",
    "            # Save each match's data\n",
    "            matches_dir = comp_dir / 'matches'\n",
    "            matches_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            for match_id, match_data in comp_data['matches'].items():\n",
    "                # Create match directory\n",
    "                match_dir = matches_dir / str(match_id)\n",
    "                match_dir.mkdir(exist_ok=True)\n",
    "                \n",
    "                # Save match metadata\n",
    "                match_meta = {\n",
    "                    'home_team': match_data['home_team'],\n",
    "                    'away_team': match_data['away_team'],\n",
    "                    'score': match_data['score'],\n",
    "                    'has_360_data': match_data['has_360_data']\n",
    "                }\n",
    "                with open(match_dir / 'metadata.pkl', 'wb') as f:\n",
    "                    pickle.dump(match_meta, f)\n",
    "                \n",
    "                # Save events data\n",
    "                with open(match_dir / 'events.pkl', 'wb') as f:\n",
    "                    pickle.dump(match_data['events'], f)\n",
    "                \n",
    "                # Save freeze frames if available\n",
    "                if match_data['has_360_data']:\n",
    "                    with open(match_dir / 'frames.pkl', 'wb') as f:\n",
    "                        pickle.dump(match_data['freeze_frames'], f)\n",
    "        \n",
    "        print(f\"Analysis dataset saved to {dataset_dir}\")\n",
    "        return str(dataset_dir)\n",
    "    \n",
    "    def load_analysis_dataset(self, dataset_path: str, load_data: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Load a previously saved analysis dataset from disk.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Path to the dataset directory\n",
    "            load_data: If True, loads all events and frames data (can be memory intensive)\n",
    "            \n",
    "        Returns:\n",
    "            Loaded analysis dataset dictionary\n",
    "        \"\"\"\n",
    "        dataset_dir = Path(dataset_path)\n",
    "        if not dataset_dir.exists():\n",
    "            raise ValueError(f\"Dataset directory {dataset_path} does not exist\")\n",
    "            \n",
    "        # Load summary\n",
    "        with open(dataset_dir / 'summary.pkl', 'rb') as f:\n",
    "            summary_data = pickle.load(f)\n",
    "            \n",
    "        # Initialize analysis data structure\n",
    "        analysis_data = {\n",
    "            'competitions': {},\n",
    "            'summary': summary_data['summary']\n",
    "        }\n",
    "        \n",
    "        # Load each competition\n",
    "        competitions_dir = dataset_dir / 'competitions'\n",
    "        for comp_id_dir in competitions_dir.iterdir():\n",
    "            if not comp_id_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            comp_id = int(comp_id_dir.name)\n",
    "            \n",
    "            # Load competition metadata\n",
    "            with open(comp_id_dir / 'metadata.pkl', 'rb') as f:\n",
    "                comp_meta = pickle.load(f)\n",
    "            \n",
    "            # Initialize competition data\n",
    "            analysis_data['competitions'][comp_id] = {\n",
    "                'name': comp_meta['name'],\n",
    "                'season': comp_meta['season'],\n",
    "                'matches': {}\n",
    "            }\n",
    "            \n",
    "            # Load matches\n",
    "            matches_dir = comp_id_dir / 'matches'\n",
    "            for match_id_dir in matches_dir.iterdir():\n",
    "                if not match_id_dir.is_dir():\n",
    "                    continue\n",
    "                    \n",
    "                match_id = int(match_id_dir.name)\n",
    "                \n",
    "                # Load match metadata\n",
    "                with open(match_id_dir / 'metadata.pkl', 'rb') as f:\n",
    "                    match_meta = pickle.load(f)\n",
    "                \n",
    "                # Initialize match data with metadata\n",
    "                match_data = {\n",
    "                    'home_team': match_meta['home_team'],\n",
    "                    'away_team': match_meta['away_team'],\n",
    "                    'score': match_meta['score'],\n",
    "                    'has_360_data': match_meta['has_360_data']\n",
    "                }\n",
    "                \n",
    "                # Optionally load events and frames data\n",
    "                if load_data:\n",
    "                    # Load events\n",
    "                    with open(match_id_dir / 'events.pkl', 'rb') as f:\n",
    "                        match_data['events'] = pickle.load(f)\n",
    "                    \n",
    "                    # Load freeze frames if available\n",
    "                    if match_meta['has_360_data'] and (match_id_dir / 'frames.pkl').exists():\n",
    "                        with open(match_id_dir / 'frames.pkl', 'rb') as f:\n",
    "                            match_data['freeze_frames'] = pickle.load(f)\n",
    "                    else:\n",
    "                        # Create empty DataFrame with expected structure\n",
    "                        match_data['freeze_frames'] = pd.DataFrame({\n",
    "                            'id': [], 'visible_area': [], 'match_id': [],\n",
    "                            'teammate': [], 'actor': [], 'keeper': [], 'location': []\n",
    "                        })\n",
    "                else:\n",
    "                    # Create placeholder objects that will be loaded on demand\n",
    "                    match_data['events'] = None\n",
    "                    match_data['freeze_frames'] = None\n",
    "                    \n",
    "                # Add match data to competition\n",
    "                analysis_data['competitions'][comp_id]['matches'][match_id] = match_data\n",
    "        \n",
    "        print(f\"Loaded analysis dataset from {dataset_path}\")\n",
    "        print(f\"Summary: {analysis_data['summary']}\")\n",
    "        return analysis_data"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "827eea2f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "Here's how to use this class to manage data for 10 competitions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b644090",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:10:34.107466Z",
     "start_time": "2025-05-24T22:07:27.765287Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "# sys.path.append('/workspaces/football-insights-api')\n",
    "\n",
    "# from src.data.football_data_manager import FootballDataManager\n",
    "\n",
    "# Initialize manager\n",
    "fdm = FootballDataManager()\n",
    "\n",
    "# Get all competitions with 360 data available\n",
    "competitions = fdm.get_competitions(only_with_360=True, exclude_women=True)\n",
    "print(f\"Found {len(competitions)} competitions with 360 data\")\n",
    "\n",
    "# Select top 10 competitions\n",
    "top_competitions = competitions.head(10)\n",
    "competition_ids = top_competitions['competition_id'].tolist()\n",
    "\n",
    "# Prepare analysis dataset with limited matches per competition\n",
    "analysis_data = fdm.prepare_data_for_analysis(\n",
    "    competition_ids=competition_ids,\n",
    "    max_matches_per_competition=3\n",
    ")\n",
    "\n",
    "# Show summary\n",
    "print(f\"\\nAnalysis Dataset Summary:\")\n",
    "print(f\"Total Competitions: {analysis_data['summary']['total_competitions']}\")\n",
    "print(f\"Total Matches: {analysis_data['summary']['total_matches']}\")\n",
    "print(f\"Total Events: {analysis_data['summary']['total_events']}\")\n",
    "print(f\"Matches with 360 data: {analysis_data['summary']['matches_with_360']}\")\n",
    "\n",
    "# Example analysis: Calculate average shots per match for each competition\n",
    "print(\"\\nAverage shots per match by competition:\")\n",
    "for comp_id, comp_data in analysis_data['competitions'].items():\n",
    "    comp_name = comp_data['name']\n",
    "    shots_per_match = []\n",
    "    matches_with_360 = 0\n",
    "    \n",
    "    for match_id, match_data in comp_data['matches'].items():\n",
    "        events_df = match_data['events']\n",
    "        shots = events_df[events_df['type'] == 'Shot'].shape[0]\n",
    "        shots_per_match.append(shots)\n",
    "        if match_data['has_360_data']:\n",
    "            matches_with_360 += 1\n",
    "    \n",
    "    avg_shots = sum(shots_per_match) / len(shots_per_match) if shots_per_match else 0\n",
    "    print(f\"{comp_name}: {avg_shots:.1f} shots per match ({matches_with_360}/{len(shots_per_match)} matches with 360 data)\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 competitions with 360 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3773386. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3773565. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3773457. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3838017. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3837987. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3837928. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3802643. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3803000. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3802802. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3877060. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3877090. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 data not available for match 3877194. Returning empty DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\miniconda3\\Lib\\site-packages\\statsbombpy\\api_client.py:21: NoAuthWarning: credentials were not supplied. open data access only\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Dataset Summary:\n",
      "Total Competitions: 10\n",
      "Total Matches: 30\n",
      "Total Events: 112395\n",
      "Matches with 360 data: 18\n",
      "\n",
      "Average shots per match by competition:\n",
      "1. Bundesliga: 27.3 shots per match (3/3 matches with 360 data)\n",
      "FIFA World Cup: 22.3 shots per match (3/3 matches with 360 data)\n",
      "La Liga: 25.3 shots per match (0/3 matches with 360 data)\n",
      "Ligue 1: 28.0 shots per match (0/3 matches with 360 data)\n",
      "Major League Soccer: 24.0 shots per match (0/3 matches with 360 data)\n",
      "UEFA Euro: 31.3 shots per match (3/3 matches with 360 data)\n",
      "UEFA Women's Euro: 23.7 shots per match (3/3 matches with 360 data)\n",
      "Women's World Cup: 23.7 shots per match (3/3 matches with 360 data)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "2fa40cfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:10:49.064107Z",
     "start_time": "2025-05-24T22:10:37.091404Z"
    }
   },
   "source": [
    "# Save the analysis dataset to disk with a custom name\n",
    "dataset_path = fdm.save_analysis_dataset(analysis_data, dataset_name=\"top_10_competitions\")\n",
    "print(f\"Dataset saved to {dataset_path}\")\n",
    "\n",
    "# Later, you can load it back (without loading all data into memory)\n",
    "loaded_data_metadata = fdm.load_analysis_dataset(dataset_path, load_data=False)\n",
    "print(\"Dataset metadata loaded (events and frames data not loaded yet)\")\n",
    "\n",
    "# Or load it with all the data\n",
    "loaded_data_complete = fdm.load_analysis_dataset(dataset_path, load_data=True)\n",
    "print(\"Complete dataset loaded with all events and frames data\")\n",
    "\n",
    "# Verify we have the same structure and data\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Original competitions: {len(analysis_data['competitions'])}\")\n",
    "print(f\"Loaded competitions: {len(loaded_data_complete['competitions'])}\")\n",
    "\n",
    "# Check the first competition's data\n",
    "first_comp_id = list(analysis_data['competitions'].keys())[0]\n",
    "original_matches = len(analysis_data['competitions'][first_comp_id]['matches'])\n",
    "loaded_matches = len(loaded_data_complete['competitions'][first_comp_id]['matches'])\n",
    "print(f\"\\nFor competition {first_comp_id}:\")\n",
    "print(f\"Original matches: {original_matches}\")\n",
    "print(f\"Loaded matches: {loaded_matches}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis dataset saved to data_cache\\top_10_competitions\n",
      "Dataset saved to data_cache\\top_10_competitions\n",
      "Loaded analysis dataset from data_cache\\top_10_competitions\n",
      "Summary: {'total_competitions': 10, 'total_matches': 30, 'total_events': 112395, 'matches_with_360': 18}\n",
      "Dataset metadata loaded (events and frames data not loaded yet)\n",
      "Loaded analysis dataset from data_cache\\top_10_competitions\n",
      "Summary: {'total_competitions': 10, 'total_matches': 30, 'total_events': 112395, 'matches_with_360': 18}\n",
      "Complete dataset loaded with all events and frames data\n",
      "\n",
      "Verification:\n",
      "Original competitions: 8\n",
      "Loaded competitions: 8\n",
      "\n",
      "For competition 9:\n",
      "Original matches: 3\n",
      "Loaded matches: 3\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "d6134af5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This approach gives you a robust way to manage football data across multiple competitions without using CSV files or a database, while still providing efficient access to the data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfdddb6",
   "metadata": {},
   "source": [
    "# Football Data Management and Analysis Workflow\n",
    "\n",
    "This notebook demonstrates how to use the `FootballDataManager` class to efficiently manage and analyze football data from StatsBomb across multiple competitions. The notebook covers:\n",
    "\n",
    "1. **Data Management Strategy**:\n",
    "   - Hierarchical memory caching\n",
    "   - Serialization for persistence\n",
    "   - Handling large datasets efficiently\n",
    "\n",
    "2. **Step-by-Step Workflow**:\n",
    "   - Retrieving competition data\n",
    "   - Filtering competitions with specific criteria (e.g., with 360° data)\n",
    "   - Fetching match, event, and 360° freeze frame data\n",
    "   - Creating analysis-ready datasets\n",
    "   - Performing sample analysis\n",
    "   - Saving and loading datasets\n",
    "\n",
    "3. **Key Features Demonstrated**:\n",
    "   - Efficient memory management\n",
    "   - Data persistence\n",
    "   - Handling data from multiple competitions\n",
    "   - Working with StatsBomb's event and 360° data\n",
    "\n",
    "## Reference Documentation\n",
    "\n",
    "For more information on the `FootballDataManager` class and its API, see the main [README.md](/workspaces/football-insights-api/README.md) file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
