{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5efb2d",
   "metadata": {},
   "source": [
    "# Football Data Management Strategy\n",
    "\n",
    "## Storage Strategy for Multiple Competitions\n",
    "\n",
    "Since you can't use CSV or a database, here's a strategy for managing data for 10 different competitions:\n",
    "\n",
    "1. **Hierarchical Memory Cache**\n",
    "   - Create a structured in-memory cache using dictionaries\n",
    "   - Organize by competition → season → match → data type (events/360)\n",
    "   - Implement selective loading and memory management\n",
    "\n",
    "2. **Serialization for Persistence**\n",
    "   - Use Python's pickle or joblib to save processed data\n",
    "   - Store in hierarchical file structure for easy retrieval\n",
    "   - Load only what's needed when performing analysis\n",
    "\n",
    "## Implementation Approach\n",
    "\n",
    "Here's a class to manage the football data ecosystem:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ddbf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:41:23.999647Z",
     "start_time": "2025-05-31T14:41:23.934129Z"
    }
   },
   "outputs": [],
   "source": [
    "# app/util/football_data_manager.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from statsbombpy import sb\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "class FootballDataManager:\n",
    "    \"\"\"\n",
    "    Manages football data across multiple competitions with in-memory caching\n",
    "    and serialization capabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cache_dir: str = \"data_cache\"):\n",
    "        \"\"\"\n",
    "        Initialize the FootballDataManager.\n",
    "\n",
    "        Args:\n",
    "            cache_dir: Directory for serialized data storage\n",
    "        \"\"\"\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # In-memory cache structure\n",
    "        self.competitions_cache = None\n",
    "        self.matches_cache = {}  # {competition_id_season_id: matches_df}\n",
    "        self.events_cache = {}   # {match_id: events_df}\n",
    "        self.frames_cache = {}   # {match_id: freeze_frame_df}\n",
    "\n",
    "    def _flatten_competitions_json(self, competitions_json: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Flatten the raw competitions JSON into a DataFrame with all relevant fields.\n",
    "        Handles both dict and tuple formats.\n",
    "        \"\"\"\n",
    "        records = []\n",
    "        for key, val in competitions_json.items():\n",
    "            # key: tuple (country, competition, season, gender)\n",
    "            # val: dict with all fields\n",
    "            record = {\n",
    "                'competition_id': val.get('competition_id'),\n",
    "                'season_id': val.get('season_id'),\n",
    "                'competition_name': val.get('competition_name'),\n",
    "                'competition_gender': val.get('competition_gender'),\n",
    "                'country_name': val.get('country_name'),\n",
    "                'competition_youth': val.get('competition_youth'),\n",
    "                'competition_international': val.get('competition_international'),\n",
    "                'season_name': val.get('season_name'),\n",
    "                'match_updated': val.get('match_updated'),\n",
    "                'match_updated_360': val.get('match_updated_360'),\n",
    "                'match_available_360': val.get('match_available_360'),\n",
    "                'match_available': val.get('match_available'),\n",
    "                # Also include tuple key fields for reference\n",
    "                # 'key_country': key[0] if len(key) > 0 else None,\n",
    "                # 'key_competition': key[1] if len(key) > 1 else None,\n",
    "                # 'key_season': key[2] if len(key) > 2 else None,\n",
    "                # 'key_gender': key[3] if len(key) > 3 else None,\n",
    "            }\n",
    "            records.append(record)\n",
    "        return pd.DataFrame(records)\n",
    "          \n",
    "\n",
    "    def _flatten_matches_json(self, matches_json: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Flatten the raw matches JSON into a DataFrame with all relevant fields.\n",
    "        \"\"\"\n",
    "        records = []\n",
    "        for key, m in matches_json.items():\n",
    "            record = {\n",
    "                'match_id': m.get('match_id'),\n",
    "                'match_date': m.get('match_date'),\n",
    "                'kick_off': m.get('kick_off'),\n",
    "                'competition_id': m.get('competition', {}).get('competition_id') if isinstance(m.get('competition'), dict) else m.get('competition_id'),\n",
    "                'competition_name': m.get('competition', {}).get('competition_name') if isinstance(m.get('competition'), dict) else m.get('competition_name'),\n",
    "                'country_name': m.get('competition', {}).get('country_name') if isinstance(m.get('competition'), dict) else m.get('country_name'),\n",
    "                'season_id': m.get('season', {}).get('season_id') if isinstance(m.get('season'), dict) else m.get('season_id'),\n",
    "                'season_name': m.get('season', {}).get('season_name') if isinstance(m.get('season'), dict) else m.get('season_name'),\n",
    "                'home_team_id': m.get('home_team', {}).get('home_team_id') if isinstance(m.get('home_team'), dict) else m.get('home_team_id'),\n",
    "                'home_team_name': m.get('home_team', {}).get('home_team_name') if isinstance(m.get('home_team'), dict) else m.get('home_team'),\n",
    "                'home_team_gender': m.get('home_team', {}).get('home_team_gender') if isinstance(m.get('home_team'), dict) else None,\n",
    "                'home_team_group': m.get('home_team', {}).get('home_team_group') if isinstance(m.get('home_team'), dict) else None,\n",
    "                'home_team_country_id': m.get('home_team', {}).get('country', {}).get('id') if isinstance(m.get('home_team', {}).get('country'), dict) else None,\n",
    "                'home_team_country_name': m.get('home_team', {}).get('country', {}).get('name') if isinstance(m.get('home_team', {}).get('country'), dict) else None,\n",
    "                'home_manager_id': m.get('home_team', {}).get('managers', [{}])[0].get('id') if m.get('home_team', {}).get('managers') else None,\n",
    "                'home_manager_name': m.get('home_team', {}).get('managers', [{}])[0].get('name') if m.get('home_team', {}).get('managers') else None,\n",
    "                'away_team_id': m.get('away_team', {}).get('away_team_id') if isinstance(m.get('away_team'), dict) else m.get('away_team_id'),\n",
    "                'away_team_name': m.get('away_team', {}).get('away_team_name') if isinstance(m.get('away_team'), dict) else m.get('away_team'),\n",
    "                'away_team_gender': m.get('away_team', {}).get('away_team_gender') if isinstance(m.get('away_team'), dict) else None,\n",
    "                'away_team_group': m.get('away_team', {}).get('away_team_group') if isinstance(m.get('away_team'), dict) else None,\n",
    "                'away_team_country_id': m.get('away_team', {}).get('country', {}).get('id') if isinstance(m.get('away_team', {}).get('country'), dict) else None,\n",
    "                'away_team_country_name': m.get('away_team', {}).get('country', {}).get('name') if isinstance(m.get('away_team', {}).get('country'), dict) else None,\n",
    "                'away_manager_id': m.get('away_team', {}).get('managers', [{}])[0].get('id') if m.get('away_team', {}).get('managers') else None,\n",
    "                'away_manager_name': m.get('away_team', {}).get('managers', [{}])[0].get('name') if m.get('away_team', {}).get('managers') else None,\n",
    "                'home_score': m.get('home_score'),\n",
    "                'away_score': m.get('away_score'),\n",
    "                'match_status': m.get('match_status'),\n",
    "                'match_status_360': m.get('match_status_360'),\n",
    "                'last_updated': m.get('last_updated'),\n",
    "                'last_updated_360': m.get('last_updated_360'),\n",
    "                'data_version': m.get('metadata', {}).get('data_version') if isinstance(m.get('metadata'), dict) else None,\n",
    "                'shot_fidelity_version': m.get('metadata', {}).get('shot_fidelity_version') if isinstance(m.get('metadata'), dict) else None,\n",
    "                'xy_fidelity_version': m.get('metadata', {}).get('xy_fidelity_version') if isinstance(m.get('metadata'), dict) else None,\n",
    "                'match_week': m.get('match_week'),\n",
    "                'competition_stage_id': m.get('competition_stage', {}).get('id') if isinstance(m.get('competition_stage'), dict) else None,\n",
    "                'competition_stage_name': m.get('competition_stage', {}).get('name') if isinstance(m.get('competition_stage'), dict) else None,\n",
    "                'stadium_id': m.get('stadium', {}).get('id') if isinstance(m.get('stadium'), dict) else None,\n",
    "                'stadium_name': m.get('stadium', {}).get('name') if isinstance(m.get('stadium'), dict) else None,\n",
    "                'stadium_country_id': m.get('stadium', {}).get('country', {}).get('id') if isinstance(m.get('stadium', {}).get('country'), dict) else None,\n",
    "                'stadium_country_name': m.get('stadium', {}).get('country', {}).get('name') if isinstance(m.get('stadium', {}).get('country'), dict) else None,\n",
    "                'referee_id': m.get('referee', {}).get('id') if isinstance(m.get('referee'), dict) else None,\n",
    "                'referee_name': m.get('referee', {}).get('name') if isinstance(m.get('referee'), dict) else None,\n",
    "                'referee_country_id': m.get('referee', {}).get('country', {}).get('id') if isinstance(m.get('referee', {}).get('country'), dict) else None,\n",
    "                'referee_country_name': m.get('referee', {}).get('country', {}).get('name') if isinstance(m.get('referee', {}).get('country'), dict) else None,\n",
    "                # Add more fields as needed\n",
    "            }\n",
    "            records.append(record)\n",
    "        return pd.DataFrame(records)\n",
    "\n",
    "    def _flatten_events_json(self, events_json: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Flatten the raw events JSON into a DataFrame with all relevant fields, including tactics and lineup.\n",
    "        \"\"\"\n",
    "        records = []\n",
    "        for key, e in events_json.items():\n",
    "            record = {\n",
    "                'id': e.get('id'),\n",
    "                'index': e.get('index'),\n",
    "                'period': e.get('period'),\n",
    "                'timestamp': e.get('timestamp'),\n",
    "                'minute': e.get('minute'),\n",
    "                'second': e.get('second'),\n",
    "                'type_id': e.get('type', {}).get('id') if isinstance(e.get('type'), dict) else None,\n",
    "                'type_name': e.get('type', {}).get('name') if isinstance(e.get('type'), dict) else e.get('type'),\n",
    "                'possession': e.get('possession'),\n",
    "                'possession_team_id': e.get('possession_team', {}).get('id') if isinstance(e.get('possession_team'), dict) else None,\n",
    "                'possession_team_name': e.get('possession_team', {}).get('name') if isinstance(e.get('possession_team'), dict) else e.get('possession_team'),\n",
    "                'play_pattern_id': e.get('play_pattern', {}).get('id') if isinstance(e.get('play_pattern'), dict) else None,\n",
    "                'play_pattern_name': e.get('play_pattern', {}).get('name') if isinstance(e.get('play_pattern'), dict) else e.get('play_pattern'),\n",
    "                'team_id': e.get('team', {}).get('id') if isinstance(e.get('team'), dict) else None,\n",
    "                'team_name': e.get('team', {}).get('name') if isinstance(e.get('team'), dict) else e.get('team'),\n",
    "                'player_id': e.get('player', {}).get('id') if isinstance(e.get('player'), dict) else None,\n",
    "                'player_name': e.get('player', {}).get('name') if isinstance(e.get('player'), dict) else e.get('player'),\n",
    "                'location': e.get('location'),\n",
    "                'duration': e.get('duration'),\n",
    "                'match_id': e.get('match_id'),\n",
    "                # Tactics fields\n",
    "                'tactics_formation': e.get('tactics', {}).get('formation') if isinstance(e.get('tactics'), dict) else None,\n",
    "                'tactics_lineup': e.get('tactics', {}).get('lineup') if isinstance(e.get('tactics'), dict) else None,\n",
    "                # Add more fields as needed, including nested ones\n",
    "            }\n",
    "            # Optionally flatten more nested fields (e.g., pass, shot, etc.)\n",
    "            if 'pass' in e:\n",
    "                record['pass_end_location'] = e['pass'].get('end_location')\n",
    "                record['pass_outcome'] = e['pass'].get('outcome', {}).get('name') if isinstance(e['pass'].get('outcome'), dict) else e['pass'].get('outcome')\n",
    "            if 'shot' in e:\n",
    "                record['shot_end_location'] = e['shot'].get('end_location')\n",
    "                record['shot_outcome'] = e['shot'].get('outcome', {}).get('name') if isinstance(e['shot'].get('outcome'), dict) else e['shot'].get('outcome')\n",
    "                record['shot_statsbomb_xg'] = e['shot'].get('statsbomb_xg')\n",
    "            # Optionally flatten lineup (store as JSON string for DataFrame compatibility)\n",
    "            if 'tactics' in e and isinstance(e['tactics'], dict) and 'lineup' in e['tactics']:\n",
    "                import json as _json\n",
    "                record['tactics_lineup_json'] = _json.dumps(e['tactics']['lineup'])\n",
    "            records.append(record)\n",
    "        df = pd.DataFrame(records)\n",
    "        if 'id' in df.columns:\n",
    "            df.set_index('id', inplace=True)\n",
    "        return df\n",
    "\n",
    "    def get_competitions(self, force_refresh: bool = False,\n",
    "                         only_with_360: bool = True,\n",
    "                         exclude_women: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get available competitions, with optional filtering.\n",
    "        \"\"\"\n",
    "        if self.competitions_cache is None or force_refresh:\n",
    "            competitions_json = sb.competitions(fmt='json')\n",
    "            competitions = self._flatten_competitions_json(competitions_json)\n",
    "            # Apply filters if specified\n",
    "            if only_with_360:\n",
    "                competitions = competitions[competitions['match_available_360'].notna()]\n",
    "            if exclude_women:\n",
    "                competitions = competitions[competitions['competition_gender'] != 'women']\n",
    "            self.competitions_cache = competitions\n",
    "        if not isinstance(self.competitions_cache, pd.DataFrame):\n",
    "            self.competitions_cache = pd.DataFrame(self.competitions_cache)\n",
    "        return self.competitions_cache\n",
    "\n",
    "    def get_matches(self, competition_id: int, season_id: int,\n",
    "                   force_refresh: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get matches for a specific competition and season.\n",
    "        \"\"\"\n",
    "        cache_key = f\"{competition_id}_{season_id}\"\n",
    "        if cache_key not in self.matches_cache or force_refresh:\n",
    "            matches_json = sb.matches(competition_id=competition_id, season_id=season_id, fmt='json')\n",
    "            matches = self._flatten_matches_json(matches_json)\n",
    "            self.matches_cache[cache_key] = matches\n",
    "        return self.matches_cache[cache_key]\n",
    "\n",
    "    def get_events(self, match_id: int, force_refresh: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get events data for a specific match.\n",
    "        \"\"\"\n",
    "        if match_id not in self.events_cache or force_refresh:\n",
    "            events_json = sb.events(match_id=match_id, fmt='json')\n",
    "            events_df = self._flatten_events_json(events_json)\n",
    "            self.events_cache[match_id] = events_df\n",
    "        return self.events_cache[match_id]\n",
    "\n",
    "    def get_freeze_frames(self, match_id: int, force_refresh: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get 360 freeze frame data for a specific match.\n",
    "\n",
    "        Args:\n",
    "            match_id: The match ID\n",
    "            force_refresh: If True, fetches from API even if cached\n",
    "\n",
    "        Returns:\n",
    "            DataFrame of freeze frame data or empty DataFrame if not available\n",
    "        \"\"\"\n",
    "        if match_id not in self.frames_cache or force_refresh:\n",
    "            try:\n",
    "                frames = sb.frames(match_id=match_id)\n",
    "                self.frames_cache[match_id] = frames\n",
    "            except (requests.exceptions.HTTPError, json.JSONDecodeError) as e:\n",
    "                # Handle 404 errors or JSON decoding errors\n",
    "                if isinstance(e, requests.exceptions.HTTPError) and e.response.status_code == 404:\n",
    "                    print(f\"360 data not available for match {match_id}. Returning empty DataFrame.\")\n",
    "                else:\n",
    "                    print(f\"Error fetching 360 data for match {match_id}: {str(e)}. Returning empty DataFrame.\")\n",
    "\n",
    "                # Create empty DataFrame with expected structure\n",
    "                self.frames_cache[match_id] = pd.DataFrame({\n",
    "                    'id': [], 'visible_area': [], 'match_id': [],\n",
    "                    'teammate': [], 'actor': [], 'keeper': [], 'location': []\n",
    "                })\n",
    "\n",
    "        return self.frames_cache[match_id]\n",
    "\n",
    "    def _process_events(self, events_data) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process events data into a standardized DataFrame.\n",
    "\n",
    "        Args:\n",
    "            events_data: Raw events data from statsbombpy\n",
    "\n",
    "        Returns:\n",
    "            Processed DataFrame\n",
    "        \"\"\"\n",
    "        # Define standard columns based on your current implementation\n",
    "        columns = ['location', 'pass_end_location', 'shot_end_location',\n",
    "                  'player', 'team', 'type', 'possession', 'play_pattern',\n",
    "                  'minute', 'second', 'period', 'timestamp', 'id',\n",
    "                  'match_id', 'pass_outcome', 'shot_outcome', 'shot_statsbomb_xg',\n",
    "                  'possession_team', 'position', 'shot_freeze_frame']\n",
    "\n",
    "        # Create DataFrame based on input type\n",
    "        if isinstance(events_data, dict):\n",
    "            df = pd.DataFrame.from_dict(events_data, orient='index')\n",
    "        else:\n",
    "            df = pd.DataFrame(events_data)\n",
    "\n",
    "        # Ensure all columns exist\n",
    "        for col in columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "\n",
    "        # Set index to event id\n",
    "        if 'id' in df.columns:\n",
    "            df.set_index('id', inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def serialize_data(self, data_type: str, identifier: str, data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Serialize data to disk.\n",
    "\n",
    "        Args:\n",
    "            data_type: Type of data ('competitions', 'matches', 'events', 'frames')\n",
    "            identifier: Identifier for the data (e.g., match_id or competition_id)\n",
    "            data: DataFrame to serialize\n",
    "        \"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        directory = self.cache_dir / data_type\n",
    "        directory.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Save data\n",
    "        file_path = directory / f\"{identifier}.pkl\"\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def load_serialized_data(self, data_type: str, identifier: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load serialized data from disk.\n",
    "\n",
    "        Args:\n",
    "            data_type: Type of data ('competitions', 'matches', 'events', 'frames')\n",
    "            identifier: Identifier for the data (e.g., match_id or competition_id)\n",
    "\n",
    "        Returns:\n",
    "            DataFrame or None if file doesn't exist\n",
    "        \"\"\"\n",
    "        file_path = self.cache_dir / data_type / f\"{identifier}.pkl\"\n",
    "\n",
    "        if not file_path.exists():\n",
    "            return None\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def prepare_data_for_analysis(self, competition_ids: Optional[List[int]] = None,\n",
    "                                max_matches_per_competition: int = 5) -> Dict:\n",
    "        \"\"\"\n",
    "        Prepare a comprehensive dataset for analysis across multiple competitions.\n",
    "\n",
    "        Args:\n",
    "            competition_ids: List of competition IDs to include (None = use all)\n",
    "            max_matches_per_competition: Maximum matches to include per competition\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with analysis-ready data\n",
    "        \"\"\"\n",
    "        analysis_data = {\n",
    "            'competitions': {},\n",
    "            'summary': {\n",
    "                'total_competitions': 0,\n",
    "                'total_matches': 0,\n",
    "                'total_events': 0,\n",
    "                'matches_with_360': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Get competitions\n",
    "        competitions = self.get_competitions()\n",
    "\n",
    "        # Filter to requested competitions if specified\n",
    "        if competition_ids:\n",
    "            competitions = competitions[competitions['competition_id'].isin(competition_ids)]\n",
    "\n",
    "        # Process each competition\n",
    "        for _, comp in competitions.iterrows():\n",
    "            comp_id = comp['competition_id']\n",
    "            season_id = comp['season_id']\n",
    "\n",
    "            # Get matches for this competition\n",
    "            matches = self.get_matches(comp_id, season_id)\n",
    "\n",
    "            # Limit number of matches if needed\n",
    "            match_subset = matches.head(max_matches_per_competition)\n",
    "\n",
    "            # Store competition data\n",
    "            analysis_data['competitions'][comp_id] = {\n",
    "                'name': comp['competition_name'],\n",
    "                'season': comp['season_name'],\n",
    "                'matches': {}\n",
    "            }\n",
    "\n",
    "            # Process each match\n",
    "            for _, match in match_subset.iterrows():\n",
    "                match_id = match['match_id']\n",
    "\n",
    "                # Get events for this match\n",
    "                events = self.get_events(match_id)\n",
    "\n",
    "                # Attempt to get frames for this match (may be empty)\n",
    "                frames = self.get_freeze_frames(match_id)\n",
    "\n",
    "                # Store match data\n",
    "                analysis_data['competitions'][comp_id]['matches'][match_id] = {\n",
    "                    'home_team': match['home_team_name'] if 'home_team_name' in match else match.get('home_team'),\n",
    "                    'away_team': match['away_team_name'] if 'away_team_name' in match else match.get('away_team'),\n",
    "                    'score': f\"{match['home_score']}-{match['away_score']}\",\n",
    "                    'events': events,\n",
    "                    'freeze_frames': frames,\n",
    "                    'has_360_data': not frames.empty\n",
    "                }\n",
    "\n",
    "                # Increment 360 data counter if available\n",
    "                if not frames.empty:\n",
    "                    analysis_data['summary']['matches_with_360'] += 1\n",
    "\n",
    "            # Update summary counts\n",
    "            analysis_data['summary']['total_competitions'] += 1\n",
    "            analysis_data['summary']['total_matches'] += len(match_subset)\n",
    "            analysis_data['summary']['total_events'] += sum(\n",
    "                len(m['events']) for m in analysis_data['competitions'][comp_id]['matches'].values()\n",
    "            )\n",
    "\n",
    "        return analysis_data\n",
    "\n",
    "    def save_analysis_dataset(self, analysis_data: Dict, dataset_name: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Save complete analysis dataset to disk in a structured format.\n",
    "\n",
    "        Args:\n",
    "            analysis_data: Analysis dataset dictionary returned by prepare_data_for_analysis\n",
    "            dataset_name: Optional name for this dataset (defaults to timestamp)\n",
    "\n",
    "        Returns:\n",
    "            Path to saved dataset directory\n",
    "        \"\"\"\n",
    "        # Create a unique name for this dataset if not provided\n",
    "        if dataset_name is None:\n",
    "            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dataset_name = f\"analysis_{timestamp}\"\n",
    "\n",
    "        # Create directory for this dataset\n",
    "        dataset_dir = self.cache_dir / dataset_name\n",
    "        dataset_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Save dataset metadata/summary\n",
    "        summary = {\n",
    "            'created_at': datetime.datetime.now().isoformat(),\n",
    "            'summary': analysis_data['summary'],\n",
    "            'competition_ids': list(analysis_data['competitions'].keys())\n",
    "        }\n",
    "        with open(dataset_dir / 'summary.pkl', 'wb') as f:\n",
    "            pickle.dump(summary, f)\n",
    "\n",
    "        # Create competitions directory\n",
    "        competitions_dir = dataset_dir / 'competitions'\n",
    "        competitions_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Save each competition's data\n",
    "        for comp_id, comp_data in analysis_data['competitions'].items():\n",
    "            # Create competition directory\n",
    "            comp_dir = competitions_dir / str(comp_id)\n",
    "            comp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            # Save competition metadata\n",
    "            comp_meta = {\n",
    "                'name': comp_data['name'],\n",
    "                'season': comp_data['season'],\n",
    "                'match_ids': list(comp_data['matches'].keys())\n",
    "            }\n",
    "            with open(comp_dir / 'metadata.pkl', 'wb') as f:\n",
    "                pickle.dump(comp_meta, f)\n",
    "\n",
    "            # Save each match's data\n",
    "            matches_dir = comp_dir / 'matches'\n",
    "            matches_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            for match_id, match_data in comp_data['matches'].items():\n",
    "                # Create match directory\n",
    "                match_dir = matches_dir / str(match_id)\n",
    "                match_dir.mkdir(exist_ok=True)\n",
    "\n",
    "                # Save match metadata\n",
    "                match_meta = {\n",
    "                    'home_team': match_data['home_team'],\n",
    "                    'away_team': match_data['away_team'],\n",
    "                    'score': match_data['score'],\n",
    "                    'has_360_data': match_data['has_360_data']\n",
    "                }\n",
    "                with open(match_dir / 'metadata.pkl', 'wb') as f:\n",
    "                    pickle.dump(match_meta, f)\n",
    "\n",
    "                # Save events data\n",
    "                with open(match_dir / 'events.pkl', 'wb') as f:\n",
    "                    pickle.dump(match_data['events'], f)\n",
    "\n",
    "                # Save freeze frames if available\n",
    "                if match_data['has_360_data']:\n",
    "                    with open(match_dir / 'frames.pkl', 'wb') as f:\n",
    "                        pickle.dump(match_data['freeze_frames'], f)\n",
    "\n",
    "        print(f\"Analysis dataset saved to {dataset_dir}\")\n",
    "        return str(dataset_dir)\n",
    "\n",
    "    def load_analysis_dataset(self, dataset_path: str, load_data: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Load a previously saved analysis dataset from disk.\n",
    "\n",
    "        Args:\n",
    "            dataset_path: Path to the dataset directory\n",
    "            load_data: If True, loads all events and frames data (can be memory intensive)\n",
    "\n",
    "        Returns:\n",
    "            Loaded analysis dataset dictionary\n",
    "        \"\"\"\n",
    "        dataset_dir = Path(dataset_path)\n",
    "        if not dataset_dir.exists():\n",
    "            raise ValueError(f\"Dataset directory {dataset_path} does not exist\")\n",
    "\n",
    "        # Load summary\n",
    "        with open(dataset_dir / 'summary.pkl', 'rb') as f:\n",
    "            summary_data = pickle.load(f)\n",
    "\n",
    "        # Initialize analysis data structure\n",
    "        analysis_data = {\n",
    "            'competitions': {},\n",
    "            'summary': summary_data['summary']\n",
    "        }\n",
    "\n",
    "        # Load each competition\n",
    "        competitions_dir = dataset_dir / 'competitions'\n",
    "        for comp_id_dir in competitions_dir.iterdir():\n",
    "            if not comp_id_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            comp_id = int(comp_id_dir.name)\n",
    "\n",
    "            # Load competition metadata\n",
    "            with open(comp_id_dir / 'metadata.pkl', 'rb') as f:\n",
    "                comp_meta = pickle.load(f)\n",
    "\n",
    "            # Initialize competition data\n",
    "            analysis_data['competitions'][comp_id] = {\n",
    "                'name': comp_meta['name'],\n",
    "                'season': comp_meta['season'],\n",
    "                'matches': {}\n",
    "            }\n",
    "\n",
    "            # Load matches\n",
    "            matches_dir = comp_id_dir / 'matches'\n",
    "            for match_id_dir in matches_dir.iterdir():\n",
    "                if not match_id_dir.is_dir():\n",
    "                    continue\n",
    "\n",
    "                match_id = int(match_id_dir.name)\n",
    "\n",
    "                # Load match metadata\n",
    "                with open(match_id_dir / 'metadata.pkl', 'rb') as f:\n",
    "                    match_meta = pickle.load(f)\n",
    "\n",
    "                # Initialize match data with metadata\n",
    "                match_data = {\n",
    "                    'home_team': match_meta['home_team'],\n",
    "                    'away_team': match_meta['away_team'],\n",
    "                    'score': match_meta['score'],\n",
    "                    'has_360_data': match_meta['has_360_data']\n",
    "                }\n",
    "\n",
    "                # Optionally load events and frames data\n",
    "                if load_data:\n",
    "                    # Load events\n",
    "                    with open(match_id_dir / 'events.pkl', 'rb') as f:\n",
    "                        match_data['events'] = pickle.load(f)\n",
    "\n",
    "                    # Load freeze frames if available\n",
    "                    if match_meta['has_360_data'] and (match_id_dir / 'frames.pkl').exists():\n",
    "                        with open(match_id_dir / 'frames.pkl', 'rb') as f:\n",
    "                            match_data['freeze_frames'] = pickle.load(f)\n",
    "                    else:\n",
    "                        # Create empty DataFrame with expected structure\n",
    "                        match_data['freeze_frames'] = pd.DataFrame({\n",
    "                            'id': [], 'visible_area': [], 'match_id': [],\n",
    "                            'teammate': [], 'actor': [], 'keeper': [], 'location': []\n",
    "                        })\n",
    "                else:\n",
    "                    # Create placeholder objects that will be loaded on demand\n",
    "                    match_data['events'] = None\n",
    "                    match_data['freeze_frames'] = None\n",
    "\n",
    "                # Add match data to competition\n",
    "                analysis_data['competitions'][comp_id]['matches'][match_id] = match_data\n",
    "\n",
    "        print(f\"Loaded analysis dataset from {dataset_path}\")\n",
    "        print(f\"Summary: {analysis_data['summary']}\")\n",
    "        return analysis_data\n",
    "\n",
    "    def get_matches_for_team(self, competition_id: int, season_id: int, team_id: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all matches for a given team in a competition and season.\n",
    "        Returns a DataFrame of matches where the team is home or away.\n",
    "        \"\"\"\n",
    "        matches = self.get_matches(competition_id, season_id)\n",
    "        return matches[(matches['home_team_id'] == team_id) | (matches['away_team_id'] == team_id)]\n",
    "\n",
    "    def get_events_for_team(self, competition_id: int, season_id: int, team_id: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all events for a given team in a competition and season.\n",
    "        Returns a DataFrame of events for all matches where the team participated.\n",
    "        \"\"\"\n",
    "        matches = self.get_matches_for_team(competition_id, season_id, team_id)\n",
    "        all_events = []\n",
    "        for _, match in matches.iterrows():\n",
    "            events = self.get_events(match['match_id'])\n",
    "            team_events = events[events['team_id'] == team_id] if 'team_id' in events else events[events['team'] == team_id]\n",
    "            all_events.append(team_events)\n",
    "        if all_events:\n",
    "            return pd.concat(all_events)\n",
    "        else:\n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827eea2f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "Here's how to use this class to manage data for 10 competitions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b644090",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:44:09.096097Z",
     "start_time": "2025-05-31T14:44:09.024891Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# At the beginning of your notebook, add:\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get configuration values with defaults\n",
    "max_competitions = int(os.environ.get(\"MAX_COMPETITIONS\", 10))\n",
    "max_matches_per_competition = int(os.environ.get(\"MAX_MATCHES_PER_COMPETITION\", 5))\n",
    "\n",
    "print(f\"Using settings: MAX_COMPETITIONS={max_competitions}, MAX_MATCHES_PER_COMPETITION={max_matches_per_competition}\")\n",
    "\n",
    "# Initialize manager\n",
    "fdm = FootballDataManager()\n",
    "\n",
    "# Get all competitions with 360 data available\n",
    "competitions = fdm.get_competitions(only_with_360=True, exclude_women=True)\n",
    "print(f\"Found {len(competitions)} competitions with 360 data\")\n",
    "\n",
    "# Select top 10 competitions\n",
    "top_competitions = competitions.head(10)\n",
    "competition_ids = top_competitions['competition_id'].tolist()\n",
    "\n",
    "# Prepare analysis dataset with limited matches per competition\n",
    "analysis_data = fdm.prepare_data_for_analysis(\n",
    "    competition_ids=competition_ids,\n",
    "    max_matches_per_competition=max_matches_per_competition\n",
    ")\n",
    "\n",
    "\n",
    "# Show summary\n",
    "print(f\"\\nAnalysis Dataset Summary:\")\n",
    "print(f\"Total Competitions: {analysis_data['summary']['total_competitions']}\")\n",
    "print(f\"Total Matches: {analysis_data['summary']['total_matches']}\")\n",
    "print(f\"Total Events: {analysis_data['summary']['total_events']}\")\n",
    "print(f\"Matches with 360 data: {analysis_data['summary']['matches_with_360']}\")\n",
    "\n",
    "# Example analysis: Calculate average shots per match for each competition\n",
    "print(\"\\nAverage shots per match by competition:\")\n",
    "for comp_id, comp_data in analysis_data['competitions'].items():\n",
    "    comp_name = comp_data['name']\n",
    "    shots_per_match = []\n",
    "    matches_with_360 = 0\n",
    "    \n",
    "    for match_id, match_data in comp_data['matches'].items():\n",
    "        events_df = match_data['events']\n",
    "        shots = events_df[events_df['type_name'] == 'Shot'].shape[0]\n",
    "        shots_per_match.append(shots)\n",
    "        if match_data['has_360_data']:\n",
    "            matches_with_360 += 1\n",
    "    \n",
    "    avg_shots = sum(shots_per_match) / len(shots_per_match) if shots_per_match else 0\n",
    "    print(f\"{comp_name}: {avg_shots:.1f} shots per match ({matches_with_360}/{len(shots_per_match)} matches with 360 data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa40cfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:41:25.740567500Z",
     "start_time": "2025-05-25T19:08:39.132621Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save the analysis dataset to disk with a custom name\n",
    "dataset_path = fdm.save_analysis_dataset(analysis_data, dataset_name=\"top_10_competitions\")\n",
    "print(f\"Dataset saved to {dataset_path}\")\n",
    "\n",
    "# Later, you can load it back (without loading all data into memory)\n",
    "loaded_data_metadata = fdm.load_analysis_dataset(dataset_path, load_data=False)\n",
    "print(\"Dataset metadata loaded (events and frames data not loaded yet)\")\n",
    "\n",
    "# Or load it with all the data\n",
    "loaded_data_complete = fdm.load_analysis_dataset(dataset_path, load_data=True)\n",
    "print(\"Complete dataset loaded with all events and frames data\")\n",
    "\n",
    "# Verify we have the same structure and data\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Original competitions: {len(analysis_data['competitions'])}\")\n",
    "print(f\"Loaded competitions: {len(loaded_data_complete['competitions'])}\")\n",
    "\n",
    "# Check the first competition's data\n",
    "first_comp_id = list(analysis_data['competitions'].keys())[0]\n",
    "original_matches = len(analysis_data['competitions'][first_comp_id]['matches'])\n",
    "loaded_matches = len(loaded_data_complete['competitions'][first_comp_id]['matches'])\n",
    "print(f\"\\nFor competition {first_comp_id}:\")\n",
    "print(f\"Original matches: {original_matches}\")\n",
    "print(f\"Loaded matches: {loaded_matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6134af5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This approach gives you a robust way to manage football data across multiple competitions without using CSV files or a database, while still providing efficient access to the data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfdddb6",
   "metadata": {},
   "source": [
    "# Football Data Management and Analysis Workflow\n",
    "\n",
    "This notebook demonstrates how to use the `FootballDataManager` class to efficiently manage and analyze football data from StatsBomb across multiple competitions. The notebook covers:\n",
    "\n",
    "1. **Data Management Strategy**:\n",
    "   - Hierarchical memory caching\n",
    "   - Serialization for persistence\n",
    "   - Handling large datasets efficiently\n",
    "\n",
    "2. **Step-by-Step Workflow**:\n",
    "   - Retrieving competition data\n",
    "   - Filtering competitions with specific criteria (e.g., with 360° data)\n",
    "   - Fetching match, event, and 360° freeze frame data\n",
    "   - Creating analysis-ready datasets\n",
    "   - Performing sample analysis\n",
    "   - Saving and loading datasets\n",
    "\n",
    "3. **Key Features Demonstrated**:\n",
    "   - Efficient memory management\n",
    "   - Data persistence\n",
    "   - Handling data from multiple competitions\n",
    "   - Working with StatsBomb's event and 360° data\n",
    "\n",
    "## Reference Documentation\n",
    "\n",
    "For more information on the `FootballDataManager` class and its API, see the main [README.md](/workspaces/football-insights-api/README.md) file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
